<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Markov Chains &mdash; Computational Creativity and Multi-Agent Systems, fall 2016 0.1a documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1a',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Computational Creativity and Multi-Agent Systems, fall 2016 0.1a documentation" href="index.html" />
    <link rel="prev" title="Getting Started" href="setup.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="markov-chains">
<h1>Markov Chains<a class="headerlink" href="#markov-chains" title="Permalink to this headline">¶</a></h1>
<p>Markov chains (MC) is a stochastic process that satisfies the Markov property
(in some terms <em>memorylessness</em>). The Markov property means, that one can give
as good predictions about the system&#8217;s future given the present state as one
could knowing the full history of the system.</p>
<p>In our case, when dealing with natural language, the state of the system is
specified number of preceding chunks of text (characters, words, etc.). For
first-order Markov chains this is the direct predecessor chunk (i.e. one word
or character).</p>
<p>To generate a Markov chain, we need to have the probability for each successor
given the current state (so called state transition probabilities). In some
cases this can be handed in advance, but in most of the interesting cases, it
is computed from a given data.</p>
<p>Next, we will show with a toy example, how first-order Markov chains can be
generated using Python. We start by creating the state transition probabilities
for the artificial data, and then generate text using these probabilities.</p>
<div class="section" id="creating-first-order-markov-chains-with-toy-data">
<h2>Creating First-Order Markov Chains with Toy Data<a class="headerlink" href="#creating-first-order-markov-chains-with-toy-data" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/assamite/cc-mas16/blob/master/week1/toy_markov.py">(full code)</a></p>
<p>First, we need to generate some data. For this example, we will use very simple
model, where we first define a distribution as a string of characters, and then
draw from that distribution the target number of items:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">random</span>
<span class="n">dist</span> <span class="o">=</span> <span class="s">&#39;Iä! Iä! Cthulhu for president!&#39;</span>
<span class="n">target</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">data</span> <span class="o">=</span> <span class="s">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="p">)])</span>
</pre></div>
</div>
<p>Next, our goal is to compute the state transition probabilities. In this
example, each character in the generated string is a state and
subsequent pairs of characters represent state transitions. That is, a string
&#8216;abc&#8217; has two state transitions, from &#8216;a&#8217; to &#8216;b&#8217; and from &#8216;b&#8217; to &#8216;c&#8217;. (In
higher order Markov chains we would be interested in longer sequences of
characters, i.e. second-order Markov chain would have one state transition,
from &#8216;ab&#8217; to &#8216;bc&#8217;.)</p>
<p>State transition probabilities can be computed using nested dictionary as a
book-keeping data structure. The outer dictionary has the current (preceding)
state as a key, and the value of that key is another (inner) dictionary, which has
the succeeding states as keys and values are the number of times the succeeding
state is observed after the preceding state. To populate the data structure is
quite forward, we go over the list of states on time, and keep count of every
predecessor-successor pair:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">transitions</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">succ</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">:</span>
        <span class="c"># Predecessor key is not yet in the dictionary, so we create new</span>
        <span class="c"># dictionary for it.</span>
        <span class="n">transitions</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">succ</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">[</span><span class="n">pred</span><span class="p">]:</span>
        <span class="c"># Successor key is not yet in the dictionary, so we start counting from</span>
        <span class="c"># one.</span>
        <span class="n">transitions</span><span class="p">[</span><span class="n">pred</span><span class="p">][</span><span class="n">succ</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c"># Otherwise we just add one to the existing value.</span>
        <span class="n">transitions</span><span class="p">[</span><span class="n">pred</span><span class="p">][</span><span class="n">succ</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.0</span>
</pre></div>
</div>
<p>Now we have information about how many times each state has directly been
succeeded by other states. Right now, we do not care if our state transitions
are not complete in the sense that zero counts are not marked in the data
structure. Next, we will sum up every state&#8217;s total number of successors:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">totals</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">succ_counts</span> <span class="ow">in</span> <span class="n">transitions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">totals</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">succ_counts</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</pre></div>
</div>
<p>With <code class="docutils literal"><span class="pre">totals</span></code>, we can now compute the probabilities for each state transition
by dividing each successor&#8217;s count for a state with the total number of
successors for that state:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">probs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">succ_counts</span> <span class="ow">in</span> <span class="n">transitions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">probs</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">succ</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">succ_counts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">probs</span><span class="p">[</span><span class="n">pred</span><span class="p">][</span><span class="n">succ</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span> <span class="o">/</span> <span class="n">totals</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span>
</pre></div>
</div>
<p>In theory, we now have the state transition probabilities which could be used
to generate text. However, before doing that, we will convert the data
structure to a more usable form by representing the probabilities with a
cumulative distribution function ordered from highest to lowest probability:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">operator</span>
<span class="n">cdfs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">succ_probs</span> <span class="ow">in</span> <span class="n">probs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">items</span> <span class="o">=</span> <span class="n">succ_probs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="c"># Sort the list by the second index in each item and reverse it from</span>
    <span class="c"># highest to lowest.</span>
    <span class="n">sorted_items</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cumulative_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">sorted_items</span><span class="p">:</span>
        <span class="n">cumulative_sum</span> <span class="o">+=</span> <span class="n">prob</span>
        <span class="n">cdf</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">c</span><span class="p">,</span> <span class="n">cumulative_sum</span><span class="p">])</span>
    <span class="n">cdf</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c"># For possible rounding errors</span>
    <span class="n">cdfs</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span> <span class="o">=</span> <span class="n">cdf</span>
</pre></div>
</div>
<p>After this, all that is left is to generate some text using our <code class="docutils literal"><span class="pre">cdfs</span></code> data
structure. We start by drawing a random character from the distribution, and
generate <em>N</em> characters overall:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">start</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">markov_chain</span> <span class="o">=</span> <span class="n">start</span>

<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">markov_chain</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">markov_chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c"># Last element of the list</span>
    <span class="n">rnd</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="c"># Random number from 0 to 1</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="n">cdfs</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span>
    <span class="n">cp</span> <span class="o">=</span> <span class="n">cdf</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c"># Go through the cdf until the cumulative probability is higher than the</span>
    <span class="c"># random number &#39;rnd&#39;.</span>
    <span class="k">while</span> <span class="n">rnd</span> <span class="o">&gt;</span> <span class="n">cp</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">cp</span> <span class="o">=</span> <span class="n">cdf</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">succ</span> <span class="o">=</span> <span class="n">cdf</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="c">#print(rnd, succ, cdf)</span>
    <span class="n">markov_chain</span> <span class="o">+=</span> <span class="n">succ</span>

<span class="k">print</span><span class="p">(</span><span class="n">markov_chain</span><span class="p">)</span>
</pre></div>
</div>
<p>That&#8217;s it! We have now computed the state transition probabilities from a toy
data set, and used them to generate new data (text). It is quite easy to alter
this example to also generate higher-order Markov chains, but that is left for
future work!</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Markov Chains</a><ul>
<li><a class="reference internal" href="#creating-first-order-markov-chains-with-toy-data">Creating First-Order Markov Chains with Toy Data</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="setup.html" title="previous chapter">Getting Started</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/markov.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Simo Linkola .
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="_sources/markov.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>